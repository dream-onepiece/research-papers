{
    "data": "<h1 style=\"height: 0px; text-align: left;\"><div style=\"text-align: center;\"><span style=\"font-family: Amatic SC;\">Human Language Modeling<\/span><\/div><div style=\"text-align: center;\"><span style=\"font-family: Amatic SC;\">using<\/span><\/div><div style=\"text-align: center;\"><span style=\"font-family: Amatic SC;\">HaRT: Human-aware Recurrent Transformers<\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: x-large; font-weight: 400;\"><br \/><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: x-large; font-weight: 400;\"><br \/><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: x-large; font-weight: 400;\">Language modeling is fundamental to NLP, with many large transformer based models becoming widespread.<\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: x-large; font-weight: 400;\"><br \/><\/span><\/div><div style=\"text-align: left;\"><div class=\"separator\" style=\"clear: both; text-align: center;\"><a href=\"https:\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEjpnI_2fSnJBmpusEOfkGnzlaDUjkCf_WvuFyOYcfcYKKDXoncEAQ7-4BKTC74eqgEJehEUVuzVJtaSnlESGaGg0beZ3SSnB-qUvC8vTSRFmYPZnPebG660pNWrfPlVo9cc22H9ZT8Xip2-VukshzcOfNIYYKzKiT2eWpvCBd-S4CPlaM40Ta1OlBV9\/s2134\/fundamental_LM.png\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" data-original-height=\"828\" data-original-width=\"2134\" src=\"https:\/\/blogger.googleusercontent.com\/img\/b\/R29vZ2xl\/AVvXsEjpnI_2fSnJBmpusEOfkGnzlaDUjkCf_WvuFyOYcfcYKKDXoncEAQ7-4BKTC74eqgEJehEUVuzVJtaSnlESGaGg0beZ3SSnB-qUvC8vTSRFmYPZnPebG660pNWrfPlVo9cc22H9ZT8Xip2-VukshzcOfNIYYKzKiT2eWpvCBd-S4CPlaM40Ta1OlBV9\/s16000\/fundamental_LM.png\" \/><\/a><\/div><div style=\"text-align: left;\"><br \/><\/div><span style=\"font-family: Roboto; font-size: large; font-weight: normal;\">Large language models treat dependent inputs as independent even when they are not.<\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large;\"><span style=\"font-weight: 400;\"><br \/><\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large;\"><span style=\"font-weight: 400;\">&lt;insert gif slide 4-5&gt;<\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto;\"><span style=\"font-weight: 400;\"><br \/><\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto;\"><span style=\"font-weight: 400;\"><span style=\"font-size: large;\">Additionally, the inherent higher order structure of language, which is words come from documents and documents come from humans, is not explicit in the language modeling tasks of large LMs.<\/span><\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto;\"><span style=\"font-weight: 400;\"><span style=\"font-size: large;\"><br \/><\/span><\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto;\"><span style=\"font-weight: 400;\"><span style=\"font-size: large;\">To address these gaps, we propose <i>Human Language Modeling<\/i> (HuLM), a language modeling task grounded in the \"natural\" generators of language, people.<\/span><\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto;\"><span style=\"font-weight: 400;\"><span style=\"font-size: large;\"><br \/><\/span><\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto;\"><span style=\"font-weight: 400;\"><span style=\"font-size: large;\">Building from the traditional language task, that is,<\/span><\/span><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto;\"><span style=\"font-weight: 400;\"><span style=\"font-size: large;\"><br \/><\/span><\/span><\/span><\/div><div style=\"text-align: center;\"><span id=\"docs-internal-guid-3008fe85-7fff-b948-2eb0-309eb0b903bd\" style=\"font-weight: normal;\"><img height=\"77\" src=\"https:\/\/lh3.googleusercontent.com\/_uLqJ0-JEIW_acEZgsmQIN2s4gZrrR9l04F512b9U0D-MFCjutsNOW9_Jm-UEpTTr174IcT0SrnUm-g-NV9xtrnQO2toPJsBNDcUZ1JJOZ0-ksndIThmOSoYzfY7WPlLec2dMmG_eQC-NLBc0b-NLg=w320-h77\" width=\"320\" \/><\/span><\/div><div style=\"text-align: center;\"><br \/><\/div><p style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large;\"><span style=\"font-weight: normal;\">In HuLM, we also condition on a user state:<\/span><\/span><\/p><div style=\"text-align: center;\"><span style=\"font-weight: normal;\"><br \/><\/span><\/div><div style=\"text-align: center;\"><span style=\"font-weight: normal;\"><span id=\"docs-internal-guid-c3735696-7fff-d3bb-7ff1-bcc5928c0c57\"><img height=\"61\" src=\"https:\/\/lh4.googleusercontent.com\/FyBZA9bcz8ZRSY1aGbPK_0oEkIdP2K36vZ9iI88l85OQO7BZBAFfkQHpEl8gMtEF-nHI2d8oB8H2Q81maWW2xPgS5SsXV9q3Vl8gDBCzHYR704ZKRmihjD1xziq9ivCDA9XB-buhUL_OadGQVGbixQ=w400-h61\" width=\"400\" \/><\/span><\/span><\/div><div style=\"text-align: left;\"><br \/><\/div><p style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large; font-weight: 400;\">But, human states are somewhat stable but not entirely static.<\/span><\/p><div style=\"text-align: left;\"><span style=\"font-weight: 400;\"><br \/><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-weight: 400;\"><br \/><\/span><\/div><div style=\"text-align: left;\"><div style=\"text-align: center;\"><img height=\"155px;\" src=\"https:\/\/lh5.googleusercontent.com\/KUP2xBBSXrXtMGAi58atqOZ2bHxQY5aB2c9yN0TWiGv42jxvAmqIBhEU4KKT1G1yVWp0J9R35-ZNCIc8jXGHAa2ttEFkVbUJOFLZkFAEjfXQ1VUmRfrozO1Nhm_Nbj3TeEeGv-4tti_n1x3lgZzGYQ\" style=\"font-weight: normal;\" width=\"468px;\" \/><\/div><span id=\"docs-internal-guid-019468ad-7fff-15ec-503c-9c56b6c5e75c\" style=\"font-weight: normal;\"><p dir=\"ltr\" style=\"line-height: 1.2; margin-bottom: 0pt; margin-top: 0pt; text-align: center;\"><span face=\"Calibri, sans-serif\" style=\"font-size: 9pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;\">(Washington Outsider, 2014)<\/span><\/p><div><span face=\"Calibri, sans-serif\" style=\"font-size: 9pt; font-variant-east-asian: normal; font-variant-numeric: normal; vertical-align: baseline; white-space: pre-wrap;\"><br \/><\/span><\/div><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-weight: 400;\"><br \/><\/span><\/div><p style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large; font-weight: 400;\">To account for this, we condition on a dynamic user state:<\/span><\/p><p style=\"text-align: center;\"><span style=\"font-family: Roboto; font-size: large; font-weight: 400;\"><img height=\"48\" src=\"https:\/\/lh4.googleusercontent.com\/k4gljbpyfHlmt1tQWRyaU30KIQ3h4J8lJfx7Wzvj2K5MFdwz_WCC9--STdGn2tWoNYngH2Q-uUowi4-PJwdNxIXGYrN66_dTV5Gz8SG1MmqW11hpi5m3JKpbfw0MplWo6NhnYko6jq02RPOaLyIdfA=w320-h48\" width=\"320\" \/><\/span><\/p><p style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large; font-weight: normal;\">To address HuLM, we introduce HaRT: Human-aware Recurrent Transformer, an auto-regressive transformer with a recurrent user state:<\/span><\/p><p style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large; font-weight: normal;\"><br \/><\/span><\/p><p style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: large; font-weight: normal;\">&lt;insert gif 9&gt;&nbsp;<\/span><\/p><div style=\"text-align: left;\"><span style=\"font-weight: 400;\"><br \/><\/span><\/div><div style=\"text-align: left;\"><span style=\"font-family: Roboto; font-size: x-large; font-weight: 400;\"><br \/><\/span><\/div><\/h1>"
}